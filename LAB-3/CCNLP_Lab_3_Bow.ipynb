{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import necessary libraries"
      ],
      "metadata": {
        "id": "tDGupXXTvpBb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4fkrUsXYZaS",
        "outputId": "f0fb67ea-0ac1-48c0-a5b7-f375e341f294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "uUf-nBoyZjrZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download required NLTK resources"
      ],
      "metadata": {
        "id": "3M26B_aNvrlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rzNelRGZqGg",
        "outputId": "5d623636-706c-491e-b8ce-6c8ba5d5b7f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take input from the user for text preprocessing"
      ],
      "metadata": {
        "id": "yCBYYNR8xiiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter the text for preprocessing: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkdpI0eMZtA1",
        "outputId": "03b4223e-8f74-445c-d9f8-a4ba33c1edee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the text for preprocessing: Natural Language Processing is a great field for research and application\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert text to lowercase (Normalization)"
      ],
      "metadata": {
        "id": "xm2yT9XRxkKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_text = text.lower()\n",
        "print(\"\\nNormalized Text:\\n\", normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG5oLWkrajs1",
        "outputId": "2a8b651d-d74d-4600-9874-957ce2e2ee48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Text:\n",
            " natural language processing is a great field for research and application\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the text into sentences"
      ],
      "metadata": {
        "id": "nXqIusuuxnQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(normalized_text)\n",
        "print(\"\\nSentence Tokenization:\\n\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Xv1auJawWG",
        "outputId": "9e2c8482-1d4b-4620-9f87-51e019aa78e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization:\n",
            " ['natural language processing is a great field for research and application']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the text into words"
      ],
      "metadata": {
        "id": "2UvoT_9SxoYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(normalized_text)\n",
        "print(\"\\nWord Tokenization:\\n\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8UDgZodazlu",
        "outputId": "fabf0f63-bb88-4713-f053-07a2a0027cdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokenization:\n",
            " ['natural', 'language', 'processing', 'is', 'a', 'great', 'field', 'for', 'research', 'and', 'application']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing punctuation from the tokenized words"
      ],
      "metadata": {
        "id": "HuwDvUKmxqv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_no_punct = [word for word in words if word not in string.punctuation]\n",
        "print(\"\\nPunctuation Removal:\\n\", words_no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVPi2P27a2da",
        "outputId": "1c25290c-89bd-4aa2-c152-f100081eaa9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation Removal:\n",
            " ['natural', 'language', 'processing', 'is', 'a', 'great', 'field', 'for', 'research', 'and', 'application']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing stopwords (common words that do not add meaning)"
      ],
      "metadata": {
        "id": "iTmK3Dnfxrkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words_no_punct if word not in stop_words]\n",
        "print(\"\\nStopword Removal:\\n\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sqKoSw_a-MM",
        "outputId": "970d29f5-7e96-4b1f-94e3-b3d6f759a51b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stopword Removal:\n",
            " ['natural', 'language', 'processing', 'great', 'field', 'research', 'application']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying stemming (reducing words to their root form)"
      ],
      "metadata": {
        "id": "jM78lTkUxt6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"\\nStemming:\\n\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3wJSrJPa_-R",
        "outputId": "259731b5-9b7b-4103-cf5d-1fc7957b2b71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming:\n",
            " ['natur', 'languag', 'process', 'great', 'field', 'research', 'applic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying lemmatization (reducing words to their base form)"
      ],
      "metadata": {
        "id": "vU4JaUE1xwIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"\\nLemmatization:\\n\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C0p01SYbfJ7",
        "outputId": "78182345-13b1-48f2-a114-4b33e20b3775"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            " ['natural', 'language', 'processing', 'great', 'field', 'research', 'application']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading additional resources for POS tagging"
      ],
      "metadata": {
        "id": "xUsty6PVx2q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CkR3iqHccqv",
        "outputId": "fcdb7b11-c72f-4789-a964-1da54474e053"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Part-of-Speech (POS) tagging on tokenized words"
      ],
      "metadata": {
        "id": "slQ1fpHIx4sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(words)\n",
        "print(\"\\nPOS Tagging:\\n\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFBXDfElbhuz",
        "outputId": "c858c6bc-3f57-41fa-c036-ac17d4c795b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging:\n",
            " [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('field', 'NN'), ('for', 'IN'), ('research', 'NN'), ('and', 'CC'), ('application', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading additional resources for Named Entity Recognition"
      ],
      "metadata": {
        "id": "2dlMvuEYx7yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aiyi2GLycxg-",
        "outputId": "e5b34163-f118-4b24-a17a-6c558ae89fc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Named Entity Recognition (NER) to identify named entities"
      ],
      "metadata": {
        "id": "RF-noLqIx-JO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_chunks = ne_chunk(pos_tags)\n",
        "print(\"\\nNamed Entity Recognition (NER):\\n\", ner_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lW6EHDVbktu",
        "outputId": "e02a1cf9-2fc5-459c-c0b4-ad5c120ddc2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entity Recognition (NER):\n",
            " (S\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  great/JJ\n",
            "  field/NN\n",
            "  for/IN\n",
            "  research/NN\n",
            "  and/CC\n",
            "  application/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the CountVectorizer for Bag of Words (BoW) model"
      ],
      "metadata": {
        "id": "Nj_nZ-cex-6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "croog6NIcsJR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit and transform the filtered text"
      ],
      "metadata": {
        "id": "-0AJItWZyFiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = vectorizer.fit_transform([\" \".join(filtered_words)])"
      ],
      "metadata": {
        "id": "e5naoNKqyIjW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display vocabulary (unique words and indices)"
      ],
      "metadata": {
        "id": "4xm0AiM5yK4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVocabulary (Word to Index Mapping):\\n\", vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7ZrcN1yOLb",
        "outputId": "250125e4-6adb-4a03-bf1e-72985ea3a79a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary (Word to Index Mapping):\n",
            " {'natural': 4, 'language': 3, 'processing': 5, 'great': 2, 'field': 1, 'research': 6, 'application': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert sparse matrix to array into a dense array\n"
      ],
      "metadata": {
        "id": "MSgAw6YRyP2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_array = bow_matrix.toarray()"
      ],
      "metadata": {
        "id": "ORPTZe8ayQvS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the BoW matrix"
      ],
      "metadata": {
        "id": "_B_MrWXjyYsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBag of Words matrix:\\n\", bow_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b1mr7WzyapQ",
        "outputId": "b0aaaad8-e3cf-40ef-9b9b-5e2eff6a631d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag of Words matrix:\n",
            " [[1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert BoW matrix to a DataFrame for better visualization"
      ],
      "metadata": {
        "id": "IqexQG1mycWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_df = pd.DataFrame(bow_array, columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag of Words Representation:\\n\", bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMRgEq6FyeXr",
        "outputId": "07f726fc-c11e-4cec-a6fa-907f2b5bb974"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag of Words Representation:\n",
            "    application  field  great  language  natural  processing  research\n",
            "0            1      1      1         1        1           1         1\n"
          ]
        }
      ]
    }
  ]
}